\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{tikz}
\let\mathdefault\relax
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{url}

\title{Title of the Paper}
\author{Motoki Nakajima \\ St. Paul's School \\ \texttt{motoki.nakajima@sps.edu}}
\date{\today}

\begin{document}

\maketitle

\section{Day 1}

\subsection{Topic Area}

I select 7. A well-defined algorithmic modification.

\subsection{Formal Research Question}

In a circular harkness table discussion, how effective is the beam search algorithm, compared to a simple greedy algorithm, in determining the table arrangement that maximizes cross-talks between students?

\subsection{Formal Problem Definition}

The problem models a circular harkness table discussion geometrically, where the cross-talk between student $i$ and student $j$ is modeled as $|v_iv_j\cos(\theta_{ij})|$, where $\theta_{ij}$ is the angle between the vector $\overrightarrow{v_i v_j}$ and $\overrightarrow{O v_i}$, and $O$ is the center of the circular table. Because we take the absolute value of the cosine, having talkative people across the table helps active participation. There are $n$ students in the discussion, where each student is given an index of normalized talkativeness $v_i$ ($0 \leq v_i \leq 1$). The goal is to find the arrangement of students around the table that maximizes the total cross-talk, which can be expressed as $\sum_{i\neq j} |v_iv_j\cos(\theta_{ij})|$.

\subsection{Hypothesis}

The beam search algorithm will yield a significantly higher total cross-talk compared to the greedy algorithm, because the greedy algorithm tend to get stuck in an inefficient local maximum, while the beam search explores wider and tend to find a better local maximum. Although the beam search cost more time, considering the small number of students in a typical harkness table discussion, the importance of maximizing the cross-talk performance outweighs the cost of time.

\subsection{Theoretical Plan}

The theoretical analysis will consist of two parts: loop invariant proof that the beam search algorithm will aways find a solution that is at least as good as the greedy algorithm, and a tight bound on the time complexity of the beam search algorithm and the greedy algorithm. The loop invariant proof will be based on the trait of the beam search algorithm that it always explores a wider range of arrangements than the greedy algorithm, while also searching through the same arrangements as the greedy algorithm. The time complexity analysis will be based on the number of arrangements explored by each algorithm, which is determined by the number of students and the beam width. The greedy algorithm has a time complexity of $O(n^2)$, while the beam search algorithm has a time complexity of $O(B \cdot n^2)$, where $B$ is the beam width. 

\subsection{Experimental Plan}

The input will be a vector of $n$ normalized talkativeness values $v_i$, which all of them will be generated randomly through \texttt{std::random\_device} in C++. The single array of input will automatically determine the size of the harkness table and the students' talkativeness, and thus completely determine all the parameters of the problem. The test will be conducted for various sizes of $n$ from 5 to 100, with increments of 5, and for various beam widths $B$ from 1 to 10, with increments of 1. 20 trials will be conducted for each combination of $n$ and $B$, and the average total cross-talk will be recorded for both the greedy algorithm and the beam search algorithm. The total cross-talk index will then be plotted by \texttt{matplotlib} in Python in three different ways: a 3d plot of total cross-talk against $n$ and $B$, a 2d plot of total cross-talk against $n$ for a fixed $B$ where the difference between the two algorithms are at largest, and a 2d plot of total cross-talk against $n$ for a fixed $B$ where the difference is smallest. The results will be analyzed to determine the effectiveness of the beam search algorithm compared to the greedy algorithm in maximizing the total cross-talk in a circular harkness table discussion.

\section{Day 2/3}

\subsection{Problem definition}

The problem models an elliptical harkness table discussion geometrically.
The score of each student's contribution to the table is defined by the student's affinity level with the other students, while considering a geometric cross-talk coefficient that depends on the angle between the students.
\[
S_i = a_{ij} \cdot k_{ij} \quad (\text{for all } j \neq i)
\]
\[
k_{ij} = d_{ij}\cdot \left( \sin^{16}(\theta_{ij}) + 0.3\cos^8(\theta_{ij}) - 4.0\cos^{64}(\tfrac{\theta_{ij}}{2})\right)
\]
$S_i$ is the score of student $i$, $a_{ij}$ is the affinity level between student $i$ and student $j$, $k_{ij}$ is the cross-talk coefficient between student $i$ and student $j$, $\theta_{ij}$ is the central angle between student $i$ and student $j$, and $d_{ij}$ is the distance between student $i$ and student $j$.
The total score of the table arrangement is defined as the sum of all scores amond the students.
\[
S = \sum_{i=1}^{n} S_i
\]
The geometrically determined cross-talk coefficient $k_{ij}$ is modeled based on feedbacks from Humanities teachers at St. Paul's School, and it is designed to capture the complex relationship between the angle of seating and the level of cross-talk.

\begin{figure}[H]
    \centering
    \input{./graphics/cross_talk_coefficient.pgf}
    \caption{Cross-talk coefficient $\tfrac{k_{ij}}{d_{ij}}$ around the table.}
    \label{fig:cross_talk}
\end{figure}

The goal of the problem is to find the arrangement of students around the elliptical table that maximizes the total score $S$.
\subsection{Pseudocode}

\begin{algorithm}[H]
\caption{Arrangement constructor, gain, and place}
\begin{algorithmic}[1]
\Function{InitArrangement}{$n$}
    \State $order \gets [-1,\ldots,-1]$ (size $n$)
    \State $seat\_taken \gets [\textbf{false},\ldots,\textbf{false}]$
    \State $student\_placed \gets [\textbf{false},\ldots,\textbf{false}]$
    \State $partial\_gain \gets n \times n$ matrix of $0.0$
    \State $current\_score \gets 0.0$
\EndFunction

\Function{Gain}{$student\_idx, seat$}
    \State \Return $2.0 \cdot partial\_gain[student\_idx][seat]$
\EndFunction

\Function{Place}{$student\_idx, seat, problem$}
    \State $current\_score \gets current\_score + \Call{Gain}{student\_idx, seat}$
    \State $order[seat] \gets student\_idx$
    \State $seat\_taken[seat] \gets \textbf{true}$
    \State $student\_placed[student\_idx] \gets \textbf{true}$
    \For{$m \gets 0$ to $n-1$}
        \If{$student\_placed[m]$} \State \textbf{continue} \EndIf
        \For{$s \gets 0$ to $n-1$}
            \If{$seat\_taken[s]$} \State \textbf{continue} \EndIf
            \State $partial\_gain[m][s] \gets partial\_gain[m][s] + affinity[m][student\_idx] \cdot$
            \Statex \hspace{1.5em}$\Call{GetCrossTalk}{problem, s, seat}$
        \EndFor
    \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Problem initialization and cross-talk lookup}
\begin{algorithmic}[1]
\Function{InitProblem}{$n, seed, aspect\_ratio$}
    \State $a \gets aspect\_ratio,\; b \gets 1.0$
    \For{$i \gets 0$ to $n-1$}
        \State $px[i] \gets a\cos(2\pi i/n)$
        \State $py[i] \gets b\sin(2\pi i/n)$
    \EndFor
    \State Initialize $cross\_talk\_coeff$ as $n\times n$ zeros
    \For{$i \gets 0$ to $n-1$}
        \For{$j \gets 0$ to $n-1$}
            \If{$i=j$} \State \textbf{continue} \EndIf
            \State $dot \gets px[i]px[j] + py[i]py[j]$
            \State $len_i \gets \sqrt{px[i]^2 + py[i]^2},\; len_j \gets \sqrt{px[j]^2 + py[j]^2}$
            \State $c_\theta \gets \Call{Clamp}{dot/(len_i len_j), -1, 1}$
            \State $\theta \gets \arccos(c_\theta)$
            \State $s \gets |\sin(\theta)|,\; c \gets |\cos(\theta)|,\; h \gets \cos(\theta/2)$
            \State $cross\_talk\_coeff[i][j] \gets s^{16} + 0.3c^8 - 4.0h^{64}$
        \EndFor
    \EndFor
    \State Initialize symmetric $affinity$ matrix using RNG seeded by $seed$
\EndFunction

\Function{GetCrossTalk}{$problem, i, j$}
    \State \Return $problem.cross\_talk\_coeff[i][j]$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{GreedySolver::solve()}
\begin{algorithmic}[1]
\Function{GreedySolve}{$problem$}
    \State $arrangement \gets \Call{InitArrangement}{problem.n}$
    \For{$step \gets 0$ to $problem.n-1$}
        \State $best\_student, best\_seat \gets -1, -1$
        \State $best\_gain \gets -10^{18}$
        \For{$k \gets 0$ to $problem.n-1$}
            \If{$arrangement.student\_placed[k]$} \State \textbf{continue} \EndIf
            \For{$s \gets 0$ to $problem.n-1$}
                \If{$arrangement.seat\_taken[s]$} \State \textbf{continue} \EndIf
                \State $g \gets \Call{Gain}{k,s}$
                \State $cur \gets arrangement.current\_score + g$
                \State $best \gets arrangement.current\_score + best\_gain$
                \If{$cur > best$ \textbf{or} ($cur = best$ and tie-break on smaller $k$, then smaller $s$)}
                    \State $best\_gain \gets g$
                    \State $best\_student \gets k$
                    \State $best\_seat \gets s$
                \EndIf
            \EndFor
        \EndFor
        \State \Call{Place}{$best\_student, best\_seat, problem$}
    \EndFor
    \State \Return $arrangement$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{BeamSearchSolver::solve()}
\begin{algorithmic}[1]
\Function{BeamSearchSolve}{$problem, beam\_width$}
    \State $beam \gets [\Call{InitArrangement}{problem.n}]$
    \For{$step \gets 0$ to $problem.n-1$}
        \State $scored \gets [\ ]$
        \For{each parent index $bi$ in $beam$}
            \For{each unplaced student $k$}
                \For{each empty seat $s$}
                    \State $score \gets beam[bi].current\_score + beam[bi].gain(k,s)$
                    \State append $(score, bi, k, s)$ to $scored$
                \EndFor
            \EndFor
        \EndFor
        \State Stable-sort $scored$ by: descending score, then ascending $(bi,k,s)$

        \State $max\_per\_parent \gets 2$
        \State $parent\_count \gets$ empty map, $selected \gets [\ ]$
        \State $greedy\_cand\_idx \gets$ first index in $scored$ with $bi=0$ (or none)
        \If{$greedy\_cand\_idx$ exists}
            \State add $scored[greedy\_cand\_idx]$ to $selected$
            \State $parent\_count[0] \gets 1$
        \EndIf

        \For{$i \gets 0$ to $|scored|-1$ while $|selected| < beam\_width$}
            \If{$i = greedy\_cand\_idx$} \State \textbf{continue} \EndIf
            \If{$parent\_count[scored[i].bi] < max\_per\_parent$}
                \State add $scored[i]$ to $selected$
                \State increment $parent\_count[scored[i].bi]$
            \EndIf
        \EndFor

        \State $new\_beam \gets [\ ]$
        \For{each candidate $c$ in $selected$}
            \State $next \gets$ copy of $beam[c.bi]$
            \State \Call{Place}{$c.student\_idx, c.seat, problem$} on $next$
            \State append $next$ to $new\_beam$
        \EndFor
        \State $beam \gets new\_beam$
    \EndFor
    \State \Return arrangement in $beam$ with maximum $current\_score$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{main() experiment sweep in \texttt{src/main.cpp}}
\begin{algorithmic}[1]
\Function{Main}{}
    \State Set constants: $N\_MIN{=}5$, $N\_MAX{=}60$, $N\_STEP{=}5$, $B\_MAX{=}20$, $TRIALS{=}50$
    \State Open CSV output file and write header
    \For{$n \gets N\_MIN$ to $N\_MAX$ step $N\_STEP$}
        \For{$trial \gets 0$ to $TRIALS-1$}
            \State $seed \gets n\cdot 10000 + trial$
            \State $problem \gets \Call{InitProblem}{n, seed, 11/7}$
            \State $greedy\_score \gets \Call{GreedySolve}{problem}.current\_score$
            \For{$B \gets 1$ to $B\_MAX$}
                \State $beam\_score \gets \Call{BeamSearchSolve}{problem, B}.current\_score$
                \State Write $(n,B,trial,greedy\_score,beam\_score)$ to CSV
            \EndFor
        \EndFor
    \EndFor
    \State Close file and return success
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Theoretical reasoning}

The beam search algorithm is the core contribution in this project. The key implementation detail in \texttt{BeamSearchSolver::solve()} is that at every step, the algorithm forcibly includes the best candidate whose parent index is $bi=0$ (stored as \texttt{greedy\_cand\_idx}) before filling the rest of the beam. Since index $0$ is recursively the greedy-descendant state from the previous step, this preserves one full greedy trajectory throughout all steps.

After step $t$, the beam contains the same partial arrangement that greedy would produce after $t$ placements.

\textbf{Base case ($t=0$).} The beam is initialized with a single empty arrangement, which is identical to greedy's initial state.

\textbf{Inductive step.} Assume the greedy partial arrangement is present at index $0$ before step $t+1$. The algorithm explicitly selects the highest-scoring child of parent $0$. Therefore, the greedy child is inserted into the next beam and remains at index $0$ in \texttt{new\_beam}. Therefore, the invariant holds for step $t+1$.

At termination ($t=n$), one complete greedy arrangement exists in the final beam. The returned solution is the maximum-score arrangement in that beam, so
\[
S_{\text{beam}} \ge S_{\text{greedy}} \quad \text{for all } B \ge 1.
\]
Thus, beam search strictly generalizes greedy in this implementation, while potentially improving quality by exploring additional high-scoring branches.

\subsection{Complexity derivation}

At each step, candidate generation scans all $B$ current arrangements and all feasible $(student, seat)$ pairs, so it creates $O(Bn^2)$ candidates. Sorting takes $O(Bn^2 \log(Bn))$ time. Expanding up to $B$ arrangements into the next beam costs $O(Bn^2)$ per step.

Each expansion performs a copy of an \texttt{Arrangement} (includes an $n\times n$ \texttt{partial\_gain} table) and one \texttt{place()} call, both bounded by $O(n^2)$. Hence expansion cost is $O(Bn^2)$ per step. Summing over all $n$ steps gives
\[
T_{\text{beam}}(n,B) = O\!\left(\sum_{r=1}^{n} Br^2 \log(Br^2)\right) + O(Bn^3)
= O\!\left(Bn^3\log(Bn)\right).
\]

For greedy, each step scans all valid $(student,seat)$ pairs and then calls \texttt{place()}, both bounded by $O(n^2)$ in this implementation, so
\[
T_{\text{greedy}}(n) = O\!\left(\sum_{r=1}^{n} r^2\right) = O(n^3).
\]

Therefore, beam search adds an approximately linear factor in $B$ (plus sorting overhead) in exchange for higher solution quality and the guarantee $S_{\text{beam}} \ge S_{\text{greedy}}$.

\subsection{Experimental plan}

To isolate the effect of beam search, we evaluate $B\in\{1,2,\ldots,20\}$ over $n\in\{5,10,\ldots,60\}$ with $50$ trials per $n$. Each trial uses a deterministic seed $seed=n\cdot 10000+trial$, and both algorithms run on the same generated \texttt{Problem}. The CSV records $(n,B,trial,greedy\_score,beam\_score)$, enabling direct paired comparisons.

Primary metrics are:
\begin{itemize}[leftmargin=*]
    \item absolute gain: $\Delta = S_{\text{beam}}-S_{\text{greedy}}$,
    \item relative gain: $\Delta_{\%}=\frac{S_{\text{beam}}-S_{\text{greedy}}}{|S_{\text{greedy}}|}\times 100$,
    \item marginal benefit curve as $B$ increases.
\end{itemize}

Because $B=1$ reproduces the greedy trajectory in this implementation, it serves as an internal correctness check for the beam framework.

\subsection{Preliminary results}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{../code/plot/improvement_vs_B.pdf}
    \caption{Preliminary results showing the average total score for greedy and beam search across different $n$ and $B$.}
    \label{fig:preliminary}
\end{figure}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
